# -*- coding: utf-8 -*-
"""Fake news detection.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1lVlGSSBc05djbbJ1mqqC-CL5b0vmEMzi

**FAKE NEWS DETECTION**

**Importing libraries**
"""

import pandas as pd
import numpy as np
import re
import string
from sklearn.model_selection import train_test_split
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.svm import LinearSVC
from sklearn.metrics import classification_report, accuracy_score
import matplotlib.pyplot as plt
import seaborn as sns

"""**Loading dataset**"""

df = pd.read_csv('/Fake.csv.zip')
print(df.head())

df.columns

"""**Data processing**"""

def clean_text(text):
    text = text.lower()
    text = re.sub(r'https?://\S+|www\.\S+', '', text)
    text = re.sub(r'<.*?>+', '', text)
    text = re.sub(r'[%s]' % re.escape(string.punctuation), '', text)
    text = re.sub(r'\n', '', text)
    text = re.sub(r'\w*\d\w*', '', text)
    return text

df['full_text'] = df['full_text'].apply(clean_text)

"""**Train-Test Split**"""

X_train, X_test, y_train, y_test = train_test_split(df['text'], df['label'], test_size=0.2, random_state=42)

"""**TF-IDF Vectorization**"""

vectorizer = TfidfVectorizer(stop_words='english', max_df=0.7)
X_train_tfidf = vectorizer.fit_transform(X_train)
X_test_tfidf = vectorizer.transform(X_test)

"""**Train a Support Vector Machine (SVM) Model
python**
"""

import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.svm import LinearSVC
from sklearn.metrics import accuracy_score, classification_report
df = pd.read_csv('/Fake.csv.zip')
df['label'] = df['subject']
df['text'] = df['text'].astype(str)
df['label'] = df['subject']
print("Label Counts:\n", df['label'].value_counts())
X = df['text']
y = df['label']

X_train, X_test, y_train, y_test = train_test_split(
    X, y, test_size=0.2, stratify=y, random_state=42)
vectorizer = TfidfVectorizer(stop_words='english', max_df=0.7)
X_train_tfidf = vectorizer.fit_transform(X_train)
X_test_tfidf = vectorizer.transform(X_test)
svm = LinearSVC()
svm.fit(X_train_tfidf, y_train)
y_pred = svm.predict(X_test_tfidf)

print("\n Accuracy:", accuracy_score(y_test, y_pred))
print("\n Classification Report:\n", classification_report(y_test, y_pred))

svm = LinearSVC()
svm.fit(X_train_tfidf, y_train)
y_pred = svm.predict(X_test_tfidf)

print("\n Accuracy:", accuracy_score(y_test, y_pred))
print("\n Classification Report:\n", classification_report(y_test, y_pred))

"""**Fake News Detection using LSTM (Deep Learning)**"""

import pandas as pd
import numpy as np
import re
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import LabelEncoder
from sklearn.metrics import classification_report, accuracy_score

from tensorflow.keras.preprocessing.text import Tokenizer
from tensorflow.keras.preprocessing.sequence import pad_sequences
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Embedding, LSTM, Dense, Dropout

"""**Loading dataset**"""

df = pd.read_csv('/Fake.csv.zip')
df['label'] = df['subject']
df['text'] = df['text'].astype(str)

"""**Cleaning and Encoding**"""

def clean_text(text):
    text = re.sub(r'[^a-zA-Z]', ' ', text)
    return text.lower()

df['text'] = df['text'].apply(clean_text)

le = LabelEncoder()
df['label_encoded'] = le.fit_transform(df['label'])

"""**Tokenization**"""

tokenizer = Tokenizer(num_words=5000)
tokenizer.fit_on_texts(df['text'])

X = tokenizer.texts_to_sequences(df['text'])
X = pad_sequences(X, maxlen=300)

y = df['label_encoded']

"""**Test-Train Split**"""

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, stratify=y, random_state=42)

"""**Building Model**"""

from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Embedding, LSTM, Dense
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import LabelEncoder
from sklearn.metrics import classification_report, accuracy_score
from tensorflow.keras.preprocessing.text import Tokenizer
from tensorflow.keras.preprocessing.sequence import pad_sequences
import re
import string
import pandas as pd
try:
    num_labels = df['label_encoded'].nunique()
except NameError:
    print("DataFrame 'df' is not defined. Please run the data loading and preprocessing cells first.")
    num_labels = 2

model = Sequential()
model.add(Embedding(input_dim=5000, output_dim=64, input_length=300))
model.add(LSTM(64, dropout=0.2, recurrent_dropout=0.2))
model.add(Dense(num_labels, activation='softmax'))

model.compile(loss='sparse_categorical_crossentropy', optimizer='adam', metrics=['accuracy'])
model.summary()

"""**Load Model**"""

model.fit(X_train, y_train, batch_size=64, epochs=4, validation_split=0.1)

"""**Evaluation**"""

y_pred_prob = model.predict(X_test)
y_pred = (y_pred_prob > 0.5).astype(int).reshape(-1)

print("\n Accuracy:", accuracy_score(y_test, y_pred))
print("\n Classification Report:\n", classification_report(y_test, y_pred))

"""**Fake News Detection using BERT**

**Install Required Libraries**
"""

!pip install transformers

"""** Load and Prepare the Dataset**"""

import pandas as pd
df = pd.read_csv("/Fake.csv.zip")
df['label'] = 0
df['text'] = df['title'].astype(str) + " " + df['text'].astype(str)
df = df[['text', 'label']].dropna()
df = df.sample(n=300, random_state=42).reset_index(drop=True)

"""**Train-Test Split**"""

from sklearn.model_selection import train_test_split

X_train, X_test, y_train, y_test = train_test_split(
    df['text'], df['label'],
    test_size=0.2,
    stratify=df['label'],
    random_state=42
)
from sklearn.model_selection import train_test_split

X_train, X_test, y_train, y_test = train_test_split(
    df['text'], df['label'], test_size=0.2, stratify=df['label'], random_state=42
)

"""**Tokenize with DistilBERT**"""

import torch
from torch.utils.data import Dataset
from transformers import DistilBertTokenizer

tokenizer = DistilBertTokenizer.from_pretrained('distilbert-base-uncased')

class NewsDataset(Dataset):
    def __init__(self, texts, labels):
        self.encodings = tokenizer(list(texts), truncation=True, padding=True, max_length=512)
        self.labels = list(labels)

    def __getitem__(self, idx):
        item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}
        item['labels'] = torch.tensor(self.labels[idx], dtype=torch.long)
        return item

    def __len__(self):
        return len(self.labels)

train_dataset = NewsDataset(X_train, y_train)
test_dataset = NewsDataset(X_test, y_test)

from transformers import DistilBertForSequenceClassification

model = DistilBertForSequenceClassification.from_pretrained('distilbert-base-uncased', num_labels=2)

"""**Load DistilBERT Model**"""

from transformers import TrainingArguments

training_args = TrainingArguments(
    output_dir='./results',
    num_train_epochs=1,
    per_device_train_batch_size=8,
    per_device_eval_batch_size=8,
    save_strategy='no',
    logging_dir='./logs',
    logging_steps=10
)

"""**Training Arguments**"""

from transformers import Trainer

trainer = Trainer(
    model=model,
    args=training_args,
    train_dataset=train_dataset,
    eval_dataset=test_dataset,
    tokenizer=tokenizer
)

trainer.train()

"""**Evaluate Model**"""

from sklearn.metrics import accuracy_score, classification_report
import numpy as np

preds = trainer.predict(test_dataset)
y_pred = np.argmax(preds.predictions, axis=1)

print("Accuracy:", accuracy_score(y_test, y_pred))
print("Report:\n", classification_report(y_test, y_pred))